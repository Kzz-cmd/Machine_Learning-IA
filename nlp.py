# -*- coding: utf-8 -*-
"""SENTIMENTOS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lx-eQm91rvevcbJ0tyIQbyEmB6rmj_rf

#Importação das Bibliotecas
"""

import numpy as np
import math
import re
import pandas as pd
from bs4 import BeautifulSoup
import nltk
import seaborn as sns
import spacy as sp
import string
import random
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences

import tensorflow as tf
tf.__version__

from tensorflow.keras import layers

"""#Pré-processamento de dados"""

cols = ['sentiment','id','date','query','user','text']

train_data = pd.read_csv('/content/train.csv',header=None,names=cols,engine='python',encoding='latin1')

data = train_data
data.drop(['id','date','query','user'],axis=1,inplace=True)

"""#Limpeza"""

x = data.iloc[:,1].values
y = data.iloc[:,0].values

y

from sklearn.model_selection import train_test_split
x, _,y, _ = train_test_split(x,y,test_size=0.05, stratify=y)
x.shape

y.shape

unique, counts = np.unique(y, return_counts=True)
unique, counts

def clean_tweets(tweet):
  tweet = BeautifulSoup(tweet,'lxml').get_text()
  print(tweet)
  tweet = re.sub(r" +",' ',tweet)
  tweet = re.sub(r'@[A-Za-z0-9]+','',tweet)
  tweet = re.sub(r'https?://[A-Za-z0-9./]+','',tweet)
  tweet = re.sub(r'[^a-zA-Z.!?]',' ',tweet)
  tweet = tweet.strip()
  return tweet

nlp = sp.load('en_core_web_sm')
stop_words = sp.lang.en.STOP_WORDS

def clean_tweets2(tweet):
  tweet = tweet.lower()
  document =  nlp(tweet)
  words = []
  for token in document:
    words.append(token.text)
  words = [word for word in words if word not in stop_words and word not in string.punctuation]
  return ' '.join([str(element) for element in words])

x_limpo = pd.read_csv('/content/tweets_limpos.csv')
x_limpo = [str(item) for item in x_limpo['tweet_limpo']]
x_limpo

for _ in range(10):
  print(x_limpo[random.randint(0,len(x_limpo) - 1)])

data_labels = y

data_labels[data_labels == 4] = 1

"""#Tokenização"""

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000,oov_token='<OOV>')
tokenizer.fit_on_texts(x_limpo)

tokenizer.word_counts

tokenizer.texts_to_sequences(["i am happy"])
tokenizer.sequences_to_texts([[374, 9037, 43]])

#data_inputs = [tokenizer.texts_to_sequences(sentence) for sentence in x_limpo]
data_inputs = tokenizer.texts_to_sequences(x_limpo)

for _ in range(10):
  print(data_inputs[random.randint(0,len(data_inputs) - 1)])

"""#Padding"""

max_len = max([len(sentence) for sentence in data_inputs])
max_len

data_input = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,maxlen=max_len,value=0,padding='post',truncating='post')

data_input

"""#Construção do Modelo"""

min_len = min(len(data_input), len(data_labels))
data_input = data_input[:min_len]
data_labels = data_labels[:min_len]
data_input

train_inputs,test_inputs,train_labels,test_labels = train_test_split(data_input,data_labels,test_size=0.3, stratify=data_labels)
print("Train class distribution:", np.unique(train_labels, return_counts=True))
print("Test class distribution:", np.unique(test_labels, return_counts=True))

#Precisam ser iguais
train_inputs.shape
train_labels.shape

class DCNN(tf.keras.Model):
  def __init__(self,vocab_size,emb_dim=128,nb_filters=50,FFN_units=512,nb_classes=5,dropout_rate=0.1,training=False,name='dcnn'):
    super(DCNN,self).__init__(name=name)
    self.embedding = layers.Embedding(vocab_size,emb_dim)
    self.bigram = layers.Conv1D(filters=nb_filters,kernel_size=2,padding='same',activation='relu')
    self.trigram = layers.Conv1D(filters=nb_filters,kernel_size=3,padding='same',activation='relu')
    self.fourgram = layers.Conv1D(filters=nb_filters,kernel_size=4,padding='same',activation='relu')
    self.pool = layers.GlobalMaxPool1D()
    self.dense_1 = layers.Dense(units=FFN_units,activation='relu')
    self.dropout = layers.Dropout(rate=dropout_rate)
    if nb_classes == 2:
      self.last_dense = layers.Dense(units=1,activation='sigmoid')
    else:
      self.last_dense = layers.Dense(units=nb_classes,activation='softmax')
  def call(self, inputs, training):
    x = self.embedding(inputs)
    x_1 = self.bigram(x)
    x_1 = self.pool(x_1)
    x_2 = self.trigram(x)
    x_2 = self.pool(x_2)
    x_3 = self.fourgram(x)
    x_3 = self.pool(x_3)
    merged = tf.concat([x_1, x_2, x_3], axis=-1)
    merged = self.dense_1(merged)
    merged = self.dropout(merged, training=training)
    output = self.last_dense(merged)
    return output

"""#Treinamento"""

vocab_size = tokenizer.num_words  # ou len(tokenizer.word_index) + 1
vocab_size

emb_dim = 256
nb_filters = 64
ffn_units = 256
nb_classes = len(set(train_labels))
batch_size = 64
nb_classes

dropout_rate = 0.5
nb_epochs = 20

dcnn = DCNN(vocab_size=vocab_size,emb_dim=emb_dim,nb_filters=nb_filters,FFN_units=ffn_units,
            nb_classes=nb_classes,dropout_rate=dropout_rate)

if nb_classes == 2:
  dcnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
else:
  dcnn.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['sparse_categorical_accuracy'])

checkpoint = './'
ckpt = tf.train.Checkpoint(dcnn)
ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint,max_to_keep=1)
if ckpt_manager.latest_checkpoint:
  ckpt.restore(ckpt_manager.latest_checkpoint)
  print('Latest checkpoint restored')

#Pegando apenas 10000 dados
subset_inputs, _, subset_labels, _ = train_test_split(
    data_inputs,
    data_labels,
    train_size=10000,
    stratify=data_labels,
    random_state=42
)
train_inputs, test_inputs, train_labels, test_labels = train_test_split(
    subset_inputs,
    subset_labels,
    test_size=0.2,
    stratify=subset_labels,
    random_state=42
)

train_inputs = pad_sequences(train_inputs, maxlen=max_len, padding='post')
test_inputs = pad_sequences(test_inputs, maxlen=max_len, padding='post')
train_inputs = np.array(train_inputs)
train_labels = np.array(train_labels)
test_inputs = np.array(test_inputs)
test_labels = np.array(test_labels)
print("train_inputs shape:", train_inputs.shape)
print("train_labels shape:", train_labels.shape)
print("test_inputs shape:", test_inputs.shape)
print("test_labels shape:", test_labels.shape)
print("Labels:", np.unique(train_labels))

history = dcnn.fit(
    train_inputs,
    train_labels,
    batch_size=64,
    epochs=nb_epochs,
    validation_split=0.2,
    )

