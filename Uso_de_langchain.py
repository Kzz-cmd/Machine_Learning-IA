# -*- coding: utf-8 -*-
"""Cópia de langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b7500c_eCH69ju-uvwwe1mqil3qDLFWn
"""

# --- CÉLULA 1: INSTALAR O OLLAMA ---

# Baixa o script de instalação do Ollama
!curl -fsSL https://ollama.com/install.sh | sh

print("Ollama instalado com sucesso!")

# --- CÉLULA 2: INICIAR O SERVIDOR E BAIXAR O MODELO ---
import os
import time

# O '&' no final executa o processo em segundo plano.
os.system('ollama serve &')

# Aguarda 5 segundos para o servidor iniciar completamente.
# Usamos time.sleep() para evitar o conflito com o event loop do Colab.
print("Aguardando o servidor Ollama iniciar...")
time.sleep(5)
print("Servidor pronto.")

# Baixa o modelo desejado (ex: llama3:8b)
# Isso pode levar alguns minutos dependendo do tamanho do modelo e da conexão do Colab
!ollama pull llama3:8b

print("\nModelo Llama 3 baixado.")

# --- CÉLULA 3: INSTALAR O LANGCHAIN ---

!pip install langchain langchain_community langchain_core

print("Bibliotecas do LangChain instaladas.")

# --- CÉLULA 4: USAR LANGCHAIN PARA INTERAGIR COM O OLLAMA ---

from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Banco vetorial ... -> contexto

contexto = "API é a sigla para Application Programming Interface, ou Interface de Programação de Aplicações. Em resumo, é um conjunto de regras e definições que permitem que dois sistemas diferentes se comuniquem entre si.,OpenWeatherMap"

# 1. Instanciar o modelo
#    O LangChain vai se conectar automaticamente ao servidor Ollama que está rodando.
#    Usamos o modelo 'llama3:8b' que baixamos.
llm = ChatOllama(model="llama3:8b")

# 2. Criar o Template do Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "Você é um assistente de guia turistico, que responde as perguntas do usuario de forma simples e intuitiva, o conteudo tem que ter topicos **topico 1**, apos o topico deve haver a epxlicação, exemplos..."),
    ("user", f"de acordo com o: {contexto}, quais são as api's mais famosas da internet")
])

# 3. Criar o Parser de Saída
output_parser = StrOutputParser()

# 4. Montar a Chain
chain = prompt | llm | output_parser

# 5. Executar a Chain e imprimir a resposta
print("Enviando pergunta para o Llama 3 via LangChain...")
resposta = chain.invoke({"contexto": contexto})

print("\n--- RESPOSTA DO MODELO ---\n")
print(resposta)

# --- CÉLULA 2: UPLOAD DE ARQUIVOS ---
from google.colab import files
import os

# Cria um diretório para salvar os arquivos
upload_dir = 'documentos_upload'
os.makedirs(upload_dir, exist_ok=True)

print(f"Por favor, faça o upload de um ou mais arquivos (.txt, .pdf).")
# Abre a interface de upload do Colab
uploaded_files = files.upload()

# Salva os arquivos no diretório criado
for filename, content in uploaded_files.items():
    with open(os.path.join(upload_dir, filename), 'wb') as f:
        f.write(content)
    print(f'Salvo: {filename}')

# Lista os arquivos no diretório para confirmar
print("\nArquivos no diretório 'documentos_upload':")
!ls -lh {upload_dir}